# Advanced-5-Class-Sentiment-Analysis-on-Yelp-Data
üåü Project Overview
This project implements a complete, industrial-grade Machine Learning pipeline to perform nuanced 5-Class Sentiment Classification (1-5 stars) on a massive, real-world customer review dataset. The focus was on demonstrating proficiency in Big Data handling (Dask), Deep Learning (Transformers), and Advanced Optimization (Weighted Loss) to push accuracy beyond typical benchmarks on a semantically noisy dataset.The project was executed entirely on Google Colab (High-RAM/GPU) to simulate a powerful cloud environment.üõ†Ô∏è Technology StackCategoryTool / LibraryPurpose in ProjectData HandlingDask, Pandas, PyArrowOut-of-core processing of multi-gigabyte JSON files; optimized I/O.Modeling CorePyTorch, Transformers (Hugging Face)Core framework for deep learning and accessing pre-trained LLMs.Model ArchitectureRoBERTa-baseState-of-the-art Transformer encoder for text classification.OptimizationScikit-learn, Custom TrainerCalculation of Class Weights to mitigate label imbalance, implemented via a custom PyTorch Trainer.EnvironmentGoogle Colab (High-RAM/GPU)Utilized for accelerated model training and memory-intensive data preparation.

üíæ Data Management & Preparation (Phase 1):
The most complex phase involved transforming the raw Yelp data into a clean, model-ready format.Source Data: Yelp Academic Dataset (review.json, $\sim$5GB JSON-per-line file) and business.json.Challenge: The data size exceeded local memory limits, and the file format (JSON-per-line) was unsuitable for direct loading.Solution (Distributed Processing):The Dask library was used with a low blocksize (20MiB) to read the massive review.json file in parallel chunks, performing memory-safe, out-of-core filtering.Reviews were filtered to a targeted subset of high-volume business categories (e.g., Restaurants, Nightlife, Health & Medical).A sample of $\sim 27,900$ high-signal reviews was extracted.I/O Safety: The final, cleaned Pandas DataFrame was written to the local Colab VM disk and then securely copied to Google Drive (Parquet format), avoiding common I/O failures associated with direct cloud writes.

üß† Modeling & Optimization (Phase 2):
The core objective was to achieve the highest possible accuracy on the challenging 5-class task.1. Classification ArchitectureModel: A RoBERTa-base model was fine-tuned for Sequence Classification with an output layer producing 5 logits (one for each star rating, mapped $1-5$ to $0-4$).Training Strategy: The training was optimized over 5 epochs using a precise learning rate ($2\text{e-}5$).2. Imbalance Mitigation (Weighted Loss)To combat the inherent class imbalance (where 5-star reviews dominate, and 1/2-star reviews are sparse):Weights Calculation: Scikit-learn's compute_class_weight(class\_weight='balanced') was used to calculate inverse frequency weights.Custom Trainer: A custom WeightedLossTrainer (inheriting from Hugging Face Trainer) was implemented to integrate these weights directly into the PyTorch CrossEntropyLoss function. This ensures the model penalizes errors on minority classes (1-star, 2-star) more heavily, improving their prediction accuracy.3. Hyperparameter Tuning (Optional)(Note: If you ran the Optuna tuning and found better parameters, mention them here. If not, state that the base run used optimized defaults.)The model was trained with initial optimized defaults (or the best parameters found by Optuna), demonstrating understanding of how to balance learning rate, batch size, and weight decay for optimal convergence.

‚úÖ Final Results :
The final accuracy on the 5-class test set reflects the difficulty of distinguishing boundary cases (like 3 vs. 4 stars) in real-world data, demonstrating the model's performance on the most nuanced task.MetricValueBaseline ComparisonTest Set Accuracy (5-Class)$\mathbf{X.XX\%}$Significantly higher than the $20\%$ random baseline.Model Saved$\mathbf{final\_model\_saved\_5class\_weighted}$Ready for deployment.Conclusion: The project successfully established a robust, scalable pipeline capable of handling large-scale unstructured data and implemented advanced techniques to address core challenges in sentiment analysis, positioning the candidate well for specialized ML roles.
